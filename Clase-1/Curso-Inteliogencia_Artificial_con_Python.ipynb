{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">#Readme--> El repositorio del curso de inteligencia artificial\n",
    ">Para futuras referencias el repositorio del curso se encuentra alojado en la siguiente liga de GitHub, da click en el URL https://github.com/joanby/ia-course "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. introducción"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Como Apenden las máquinas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.1. Aprendizaje por refuerso \n",
    "\n",
    "En este tipo de aprendizaje se se ruefuerza la accion correcta, de esta forma reforzamos (aprender) la accion correcta, de la mismamanera que se entrena a un mascota animando el comportamiento deseado y penalizando el comportamiento negativo, cada accion recibe una respuesta / recompensa en funcion de su desempeño generando la tarea objetivo, el aprendizaje por refuerzo ayuda a generar conocimiento y de esta forma va aprendiendo y mejorando cada aspecto de su aprendizaje.\n",
    "\n",
    "Estricatamente el Prendizaje por refuerzo es un area del aprendizaje automatico inspirado en la psicologia conductista, cuya ocupacion es determinar que accion debe escojer un agente de software en un entorno definido con el fin de maximizar alguna nocuin de \"recompenza\" o \"premio\" acumulado. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Ecuación de BELLMAN\n",
    "\n",
    "Notaciones:\n",
    "\n",
    "- s -> estados\n",
    "- a -> acciones\n",
    "  - son las acicones que toma el algoridmo para llegar a su objetivo\n",
    "- R -> recompensas\n",
    "  -  Premio y castigo/penailzacion\n",
    "- γ -> descuento\n",
    "\n",
    ">Esta es la ecuacion de BELLMAN\n",
    "> Referencia URL: https://www.ams.org/journals/bull/1954-60-06/S0002-9904-1954-09848-8/S0002-9904-1954-09848-8.pdf\n",
    ">\n",
    ">$$ \n",
    ">V(s)=\\max _{a_{0}}(R(s,a)+γV(s')) \n",
    ">$$\n",
    ">\n",
    ">De momento parece que solo hemos hecho el problema más complicado al separar la decisión de hoy de las decisiones futuras. Pero podemos simplificar por darse cuenta de que lo que está dentro de los corchetes de la derecha es el valor del tiempo de problema de decisión 1, a partir de un estado\n",
    ">\n",
    ">$$ s_1=T(s_0,a_0)$$\n",
    ">\n",
    ">Por lo tanto se puede reescribir el problema como un recurrente definición de la función de valor:\n",
    ">\n",
    ">$$ V(s_{0})=\\max _{a_{0}}\\{R(s_{0},a_{0})+\\gamma V(s_{1})\\}$$\n",
    ">\n",
    ">sujeto a la restricción:\n",
    ">\n",
    ">$$ a_{0}\\in \\Gamma (s_{0}),\\;s_{1}=T(s_{0},a_{0}).$$\n",
    ">\n",
    ">Esta es la ecuación de Bellman. Se puede simplificar aún más si se cae subíndices de tiempo y el enchufe en el valor del siguiente estado:\n",
    ">\n",
    ">$$ V(s)=\\max _{a\\in \\Gamma (s)}\\{R(s,a)+\\gamma V(T(s,a))\\}.$$\n",
    ">\n",
    ">La ecuación de Bellman se clasifica como una ecuación funcional, porque resolver que significa la búsqueda de la función desconocida V, que es la función de valor. Recordemos que la función de valor describe el mejor valor posible del objetivo, como una función del estado s. Mediante el cálculo de la función de valor, también se encuentra la función a (s) que describe la acción óptima en función de la situación, lo que se llama a la función política.\n",
    "\n",
    "El Principio de optimalidad de Bellman es un principio aplicado en programación dinámica que consiste en que una secuencia óptima de decisiones que resuelve un problema debe cumplir la propiedad de que cualquier subsecuencia de decisiones, que tenga el mismo estado final, debe ser también óptima respecto al subproblema correspondiente. En otras palabras, una política óptima tiene la propiedad de que cualquiera que sea el estado inicial y la decisión inicial, las decisiones restantes deben constituir una política óptima en relación con el estado resultante de la primera decisión.\n",
    "\n",
    "Un ejemplo sencillo del Principio de Optimalidad de Bellman es el siguiente: si tenemos un camino mínimo de A a B pasando por C, entonces los trozos de camino de A a C y de C a B deben ser también mínimos. Esto significa que cualquier subsecuencia de una secuencia óptima debe ser, a su vez, una secuencia óptima "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Proceso de decisión de Markov\n",
    "\n",
    ">Markov Decision Process\n",
    ">se llama de Markov cuando cumple la propiedad de marckov, es decir se refiere a la propiedad de ciertos procesos estocasticos los cuales carecen de memoria, lo que significa que la distribucion de la probabilidad del valor futuro de una variable aleatoria depende unicamente de su valor presente, siendo independiente de la historia de dicha variable. A los procesos que satisfacen esta condicion se les conoce como procesos de Markov.\n",
    ">\n",
    ">es un proceso donde a cada paso del tiempo la distribicion de probabilidad de los tiempos pasados es completamente independiente, es decir todo lo que realizamos en el pasado no influye en el futuro, la eleccion que se toma solo depende del precente y no de las acciones tomadas en el pasado, cada decicion es independiente, son aleatorias o estocasticas dependeindo de la distribucion de probabilidad de en ese intante para las deciciones\n",
    "\n",
    "Para poder emplear la ventajas del proceso de decicion de Markov, tenemos que considerarlo en las ecuaciones de Belman a fin de prober de mayor livertar y aleatoriedad, asi como de un proceso de dereccion estocastico a la IA\n",
    "\n",
    ">$$ \n",
    ">V(s)=\\max _{a_{0}}(R(s,a)+γV(s')) \n",
    ">$$\n",
    "\n",
    "Recordemos que la ecuacion de Bellman maximisa la capasidad de llegar al valor maximo, para pasar al estado siguiente y llegar al valor maximo posible, de esta forma se se maximisa el el cambio de estado que favorece la llegada al estado maximo mediante un un factor de desciento y recompensa, MAXIMIZAR VALORES\n",
    "\n",
    ">Aplicacion de procesos de decicion de Markov en la ecuacion de Bellman\n",
    ">\n",
    ">pasamos de la ecuacion original\n",
    ">\n",
    ">$$ \n",
    ">V(s)=\\max _{a_{0}}(R(s,a)+γV(s'))\n",
    ">$$\n",
    ">a considerar todos los estados de decicion posibles desde la posicion de decicion hacia el nuevo estado\n",
    ">$$\n",
    ">V(s')=(V(s'_1)+V(s'_2)+V(s'_3))\n",
    ">$$\n",
    ">A la consideracion de una distribucion de probabilidad estocastica, (valores de ejemplo)\n",
    ">$$\n",
    ">V(s')=0.1*V(s'_1)+0.8*V(s'_2)+0.1*V(s'_3)\n",
    ">$$\n",
    ">Finalizando con la consideracion de la ponderacion de las posibilidades en el momento de decisión\n",
    ">$$\n",
    ">V(s)=\\max _{a_{0}}(R(s,a)+γ*\\sum_{s'} P(s,a,s')*V(s'))\n",
    ">$$\n",
    "\n",
    "ahora se integra la ponderacion o media ponderada de las probavilidades por los valores $ \\sum_{s'} P(s,a,s') $ de todas las opciones posibles para llegas del estado $ s $ a $s'$ siendo un proceso aleatorio, reparten el valor o posibles valores en la decicion actiual a la hora de maximisar la accion, es decir la probabilidad de ir del estado $s$ con una accion $a$ al estado $s'$ influira el valor final --> $P(s,a,s')$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Politica o Plan\n",
    "\n",
    "Al aplicar la un plan se tendra una ruta definida, por lo cual se considera determinista por lo los estados estaran definidos en todo momento. Al proporcionar aleatoriedad en las decciiones y manejar un distribucion de probabilidad estocastica en cada decicion de forma independeinte, considerando unicamente las condiciones en el estado donde de encuentra, se considera que se establece una politica de accion en donde de favorece la ruta con la mejor probabilidad sin tener un plan a seguir, es decir la politica establece las consideraciones de seleccion de ruta en base al objetivo que maximise la probabilidad estocastica en cada decicion de forma independeinte."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4. Factor de penalización (Living Penalty)\n",
    "\n",
    "El factor de penalización es una estrategia mediante la cual se penalizan el numero de pasos requeridos para llegar del estado $s$ al $s'$, es decir se busca que el algoridmo establesca una ruta con el menor numero de pasos $a$ a fin de logarar el objetivo lo mas eficientemente posible, es decir dependiendo el valor de recompenza tendremos un comportamiento que busque la minima penalizacion, incluso considerar aceptable llegar a un estado no deseado con tal de reducir el numero de pasos, por lo tanto el factor de penalización es una estrategia que se utiliza para evitar que el algoritmo sea capaz de encontrar una ruta en donde la probabilidad estocastica sea mayor que la probabilidad determinista, es decir que la recompensa de alcanzar el estado deseado sea mayor a la penalizacion acumulada al por el numero de pasos hasta logar el estado deseado $s'$ por tanto se llama conto de vida o Living Penalty, por lo que el algoritno buscara la ruta mas eficiente."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Q-learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

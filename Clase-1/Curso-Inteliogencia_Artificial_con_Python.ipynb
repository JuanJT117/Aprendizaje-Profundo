{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">#Readme--> El repositorio del curso de inteligencia artificial\n",
    ">Para futuras referencias el repositorio del curso se encuentra alojado en la siguiente liga de GitHub, da click en el URL https://github.com/joanby/ia-course "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. introducción"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Como Apenden las máquinas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.1. Aprendizaje por refuerso \n",
    "\n",
    "En este tipo de aprendizaje se se ruefuerza la accion correcta, de esta forma reforzamos (aprender) la accion correcta, de la mismamanera que se entrena a un mascota animando el comportamiento deseado y penalizando el comportamiento negativo, cada accion recibe una respuesta / recompensa en funcion de su desempeño generando la tarea objetivo, el aprendizaje por refuerzo ayuda a generar conocimiento y de esta forma va aprendiendo y mejorando cada aspecto de su aprendizaje.\n",
    "\n",
    "Estricatamente el Prendizaje por refuerzo es un area del aprendizaje automatico inspirado en la psicologia conductista, cuya ocupacion es determinar que accion debe escojer un agente de software en un entorno definido con el fin de maximizar alguna nocuin de \"recompenza\" o \"premio\" acumulado. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Ecuación de BELLMAN\n",
    "\n",
    "Notaciones:\n",
    "\n",
    "- s -> estados\n",
    "- a -> acciones\n",
    "  - son las acicones que toma el algoridmo para llegar a su objetivo\n",
    "- R -> recompensas\n",
    "  -  Premio y castigo/penailzacion\n",
    "- γ -> descuento\n",
    "\n",
    ">Esta es la ecuacion de BELLMAN\n",
    "> Referencia URL: https://www.ams.org/journals/bull/1954-60-06/S0002-9904-1954-09848-8/S0002-9904-1954-09848-8.pdf\n",
    ">\n",
    ">$$ \n",
    ">V(s)=\\max _{a_{0}}(R(s,a)+γV(s')) \n",
    ">$$\n",
    ">\n",
    ">De momento parece que solo hemos hecho el problema más complicado al separar la decisión de hoy de las decisiones futuras. Pero podemos simplificar por darse cuenta de que lo que está dentro de los corchetes de la derecha es el valor del tiempo de problema de decisión 1, a partir de un estado\n",
    ">\n",
    ">$$ s_1=T(s_0,a_0)$$\n",
    ">\n",
    ">Por lo tanto se puede reescribir el problema como un recurrente definición de la función de valor:\n",
    ">\n",
    ">$$ V(s_{0})=\\max _{a_{0}}\\{R(s_{0},a_{0})+\\gamma V(s_{1})\\}$$\n",
    ">\n",
    ">sujeto a la restricción:\n",
    ">\n",
    ">$$ a_{0}\\in \\Gamma (s_{0}),\\;s_{1}=T(s_{0},a_{0}).$$\n",
    ">\n",
    ">Esta es la ecuación de Bellman. Se puede simplificar aún más si se cae subíndices de tiempo y el enchufe en el valor del siguiente estado:\n",
    ">\n",
    ">$$ V(s)=\\max _{a\\in \\Gamma (s)}\\{R(s,a)+\\gamma V(T(s,a))\\}.$$\n",
    ">\n",
    ">La ecuación de Bellman se clasifica como una ecuación funcional, porque resolver que significa la búsqueda de la función desconocida V, que es la función de valor. Recordemos que la función de valor describe el mejor valor posible del objetivo, como una función del estado s. Mediante el cálculo de la función de valor, también se encuentra la función a (s) que describe la acción óptima en función de la situación, lo que se llama a la función política.\n",
    "\n",
    "El Principio de optimalidad de Bellman es un principio aplicado en programación dinámica que consiste en que una secuencia óptima de decisiones que resuelve un problema debe cumplir la propiedad de que cualquier subsecuencia de decisiones, que tenga el mismo estado final, debe ser también óptima respecto al subproblema correspondiente. En otras palabras, una política óptima tiene la propiedad de que cualquiera que sea el estado inicial y la decisión inicial, las decisiones restantes deben constituir una política óptima en relación con el estado resultante de la primera decisión.\n",
    "\n",
    "Un ejemplo sencillo del Principio de Optimalidad de Bellman es el siguiente: si tenemos un camino mínimo de A a B pasando por C, entonces los trozos de camino de A a C y de C a B deben ser también mínimos. Esto significa que cualquier subsecuencia de una secuencia óptima debe ser, a su vez, una secuencia óptima "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Proceso de decisión de Markov\n",
    "\n",
    ">Markov Decision Process\n",
    ">se llama de Markov cuando cumple la propiedad de marckov, es decir se refiere a la propiedad de ciertos procesos estocasticos los cuales carecen de memoria, lo que significa que la distribucion de la probabilidad del valor futuro de una variable aleatoria depende unicamente de su valor presente, siendo independiente de la historia de dicha variable. A los procesos que satisfacen esta condicion se les conoce como procesos de Markov.\n",
    ">\n",
    ">es un proceso donde a cada paso del tiempo la distribicion de probabilidad de los tiempos pasados es completamente independiente, es decir todo lo que realizamos en el pasado no influye en el futuro, la eleccion que se toma solo depende del precente y no de las acciones tomadas en el pasado, cada decicion es independiente, son aleatorias o estocasticas dependeindo de la distribucion de probabilidad de en ese intante para las deciciones\n",
    "\n",
    "Para poder emplear la ventajas del proceso de decicion de Markov, tenemos que considerarlo en las ecuaciones de Belman a fin de prober de mayor livertar y aleatoriedad, asi como de un proceso de dereccion estocastico a la IA\n",
    "\n",
    ">$$ \n",
    ">V(s)=\\max _{a_{0}}(R(s,a)+γV(s')) \n",
    ">$$\n",
    "\n",
    "Recordemos que la ecuacion de Bellman maximisa la capasidad de llegar al valor maximo, para pasar al estado siguiente y llegar al valor maximo posible, de esta forma se se maximisa el el cambio de estado que favorece la llegada al estado maximo mediante un un factor de desciento y recompensa, MAXIMIZAR VALORES\n",
    "\n",
    ">Aplicacion de procesos de decicion de Markov en la ecuacion de Bellman\n",
    ">\n",
    ">pasamos de la ecuacion original\n",
    ">\n",
    ">$$ \n",
    ">V(s)=\\max _{a_{0}}(R(s,a)+γV(s'))\n",
    ">$$\n",
    ">a considerar todos los estados de decicion posibles desde la posicion de decicion hacia el nuevo estado\n",
    ">$$\n",
    ">V(s')=(V(s'_1)+V(s'_2)+V(s'_3))\n",
    ">$$\n",
    ">A la consideracion de una distribucion de probabilidad estocastica, (valores de ejemplo)\n",
    ">$$\n",
    ">V(s')=0.1*V(s'_1)+0.8*V(s'_2)+0.1*V(s'_3)\n",
    ">$$\n",
    ">Finalizando con la consideracion de la ponderacion de las posibilidades en el momento de decisión\n",
    ">$$\n",
    ">V(s)=\\max _{a_{0}}(R(s,a)+γ*\\sum_{s'} P(s,a,s')*V(s'))\n",
    ">$$\n",
    "\n",
    "ahora se integra la ponderacion o media ponderada de las probavilidades por los valores $ \\sum_{s'} P(s,a,s') $ de todas las opciones posibles para llegas del estado $ s $ a $s'$ siendo un proceso aleatorio, reparten el valor o posibles valores en la decicion actiual a la hora de maximisar la accion, es decir la probabilidad de ir del estado $s$ con una accion $a$ al estado $s'$ influira el valor final --> $P(s,a,s')$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Politica o Plan\n",
    "\n",
    "Al aplicar la un plan se tendra una ruta definida, por lo cual se considera determinista por lo los estados estaran definidos en todo momento. Al proporcionar aleatoriedad en las decciiones y manejar un distribucion de probabilidad estocastica en cada decicion de forma independeinte, considerando unicamente las condiciones en el estado donde de encuentra, se considera que se establece una politica de accion en donde de favorece la ruta con la mejor probabilidad sin tener un plan a seguir, es decir la politica establece las consideraciones de seleccion de ruta en base al objetivo que maximise la probabilidad estocastica en cada decicion de forma independeinte."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4. Factor de penalización (Living Penalty)\n",
    "\n",
    "El factor de penalización es una estrategia mediante la cual se penalizan el numero de pasos requeridos para llegar del estado $s$ al $s'$, es decir se busca que el algoridmo establesca una ruta con el menor numero de pasos $a$ a fin de logarar el objetivo lo mas eficientemente posible, es decir dependiendo el valor de recompenza tendremos un comportamiento que busque la minima penalizacion, incluso considerar aceptable llegar a un estado no deseado con tal de reducir el numero de pasos, por lo tanto el factor de penalización es una estrategia que se utiliza para evitar que el algoritmo sea capaz de encontrar una ruta en donde la probabilidad estocastica sea mayor que la probabilidad determinista, es decir que la recompensa de alcanzar el estado deseado sea mayor a la penalizacion acumulada al por el numero de pasos hasta logar el estado deseado $s'$ por tanto se llama conto de vida o Living Penalty, por lo que el algoritno buscara la ruta mas eficiente."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Q-learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-1.24.3-cp310-cp310-win_amd64.whl (14.8 MB)\n",
      "     ---------------------------------------- 14.8/14.8 MB 4.1 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.24.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script f2py.exe is installed in 'c:\\Users\\juanj\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (22.3.1)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts pip.exe, pip3.10.exe and pip3.exe are installed in 'c:\\Users\\juanj\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting pip\n",
      "  Downloading pip-23.1.2-py3-none-any.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 4.9 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.3.1\n",
      "    Uninstalling pip-22.3.1:\n",
      "      Successfully uninstalled pip-22.3.1\n",
      "Successfully installed pip-23.1.2\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# AI for Logistics - Robots in a warehouse\n",
    "\n",
    "# Importacion de librerias\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#parametros de inicializacion Q-learning\n",
    "gamma = 0.75 #factor de descuento \\gamma\n",
    "alpha = 0.9 #factor de aprendizaje \\alpha\n",
    "\n",
    "# PART 1 - BUILDING THE ENVIRONMENT\n",
    "\n",
    "# definicion de cada estado de s a s' \n",
    "ubicacion_a_estado = {'A': 0,'B': 1,'C': 2,'D': 3,'E': 4,'F': 5, 'G': 6, 'H': 7,'I': 8,'J': 9,'K': 10,'L': 11}\n",
    "\n",
    "#definicion de acciones\n",
    "acciones = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "\n",
    "#definicion de recompensas \n",
    "recompensa = np.array([\n",
    "    [0,1,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,1,0,1,0,0,0,0,1,0,0],\n",
    "    [0,1,0,0,0,1,0,0,0,0,0,1],\n",
    "    [0,0,0,1,0,1,0,0,0,0,0,0],\n",
    "    [0,0,0,0,1,0,1,0,0,0,1,0],\n",
    "    [0,0,0,0,0,1,0,1,0,0,0,1],\n",
    "    [0,0,0,0,0,1,0,0,1,0,0,0],\n",
    "    [0,0,0,0,0,1,0,0,0,0,0,0],\n",
    "    [1,0,1,0,1,0,0,0,0,0,0,1],\n",
    "    [0,0,0,0,0,1,0,0,1,0,1,0],\n",
    "    [0,0,0,0,0,0,0,0,0,1,0,1],\n",
    "    [0,0,0,0,0,0,0,1,0,0,1,0]\n",
    "])\n",
    "\n",
    "# PART 2 - BUILDING THE AI SOLUTION WITH Q-LEARNING\n",
    "\n",
    "#Maps indices to locations\n",
    "estado_a_ubicacion = dict((state,location) for location,state in ubicacion_a_estado.items())\n",
    "\n",
    "def route(starting_location, ending_location):\n",
    "    R_new = np.copy(recompensa)\n",
    "    \n",
    "    ending_state = ubicacion_a_estado[ending_location]\n",
    "    R_new[ending_state, ending_state] = 1000\n",
    "    Q = np.array(np.zeros([12,12]))\n",
    "    \n",
    "    for i in range(1000):\n",
    "        current_state = np.random.randint(0,12)\n",
    "        playable_actions = []\n",
    "        for j in range(12):\n",
    "            if R_new[current_state, j] > 0:\n",
    "                playable_actions.append(j)\n",
    "        next_state = np.random.choice(playable_actions)\n",
    "        TD = R_new[current_state, next_state] + gamma * Q[next_state, np.argmax(Q[next_state,])] - Q[current_state, next_state]\n",
    "        Q[current_state, next_state] = Q[current_state, next_state] + alpha * TD\n",
    "    route = [starting_location]\n",
    "    next_location = starting_location\n",
    "    while (next_location != ending_location):\n",
    "        starting_state = ubicacion_a_estado[starting_location]\n",
    "        next_state = np.argmax(Q[starting_state,])\n",
    "        next_location = ubicacion_a_estado[next_state]\n",
    "        route.append(next_location)\n",
    "        starting_location = next_location\n",
    "    return route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Route:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mRoute:\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m route(\u001b[39m'\u001b[39;49m\u001b[39mA\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mF\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[16], line 61\u001b[0m, in \u001b[0;36mroute\u001b[1;34m(starting_location, ending_location)\u001b[0m\n\u001b[0;32m     59\u001b[0m starting_state \u001b[39m=\u001b[39m ubicacion_a_estado[starting_location]\n\u001b[0;32m     60\u001b[0m next_state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(Q[starting_state,])\n\u001b[1;32m---> 61\u001b[0m next_location \u001b[39m=\u001b[39m ubicacion_a_estado[next_state]\n\u001b[0;32m     62\u001b[0m route\u001b[39m.\u001b[39mappend(next_location)\n\u001b[0;32m     63\u001b[0m starting_location \u001b[39m=\u001b[39m next_location\n",
      "\u001b[1;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "print('Route:')\n",
    "route('A', 'F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Route:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "10",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39m# Printing the final route\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mRoute:\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m best_route(\u001b[39m'\u001b[39;49m\u001b[39mE\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mK\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mG\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m, in \u001b[0;36mbest_route\u001b[1;34m(starting_location, intermediary_location, ending_location)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbest_route\u001b[39m(starting_location, intermediary_location, ending_location):\n\u001b[1;32m----> 5\u001b[0m     \u001b[39mreturn\u001b[39;00m route(starting_location, intermediary_location) \u001b[39m+\u001b[39m route(intermediary_location, ending_location)[\u001b[39m1\u001b[39m:]\n",
      "Cell \u001b[1;32mIn[11], line 61\u001b[0m, in \u001b[0;36mroute\u001b[1;34m(starting_location, ending_location)\u001b[0m\n\u001b[0;32m     59\u001b[0m starting_state \u001b[39m=\u001b[39m ubicacion_a_estado[starting_location]\n\u001b[0;32m     60\u001b[0m next_state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(Q[starting_state,])\n\u001b[1;32m---> 61\u001b[0m next_location \u001b[39m=\u001b[39m ubicacion_a_estado[next_state]\n\u001b[0;32m     62\u001b[0m route\u001b[39m.\u001b[39mappend(next_location)\n\u001b[0;32m     63\u001b[0m starting_location \u001b[39m=\u001b[39m next_location\n",
      "\u001b[1;31mKeyError\u001b[0m: 10"
     ]
    }
   ],
   "source": [
    "# PART 3 - GOING INTO PRODUCTION\n",
    "\n",
    "# Making the final function that returns the optimal route\n",
    "def best_route(starting_location, intermediary_location, ending_location):\n",
    "    return route(starting_location, intermediary_location) + route(intermediary_location, ending_location)[1:]\n",
    "\n",
    "# Printing the final route\n",
    "print('Route:')\n",
    "best_route('E', 'K', 'G')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalacion de entorno mediante de PIP install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.12.0 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.10)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (16.0.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\juanj\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.23.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\juanj\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.6.2)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.54.2)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow) (0.40.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: scipy>=1.7 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (1.10.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.19.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.3.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package     Version Latest Type\n",
      "----------- ------- ------ -----\n",
      "gast        0.4.0   0.5.4  wheel\n",
      "numpy       1.23.5  1.24.3 wheel\n",
      "pyzmq       25.0.2  25.1.0 wheel\n",
      "setuptools  65.5.0  67.8.0 wheel\n",
      "tensorboard 2.12.3  2.13.0 wheel\n",
      "urllib3     1.26.16 2.0.2  wheel\n",
      "wrapt       1.14.1  1.15.0 wheel\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list --outdated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "                                              0.0/721.7 kB ? eta -:--:--\n",
      "     ---                                   61.4/721.7 kB 812.7 kB/s eta 0:00:01\n",
      "     -------                                143.4/721.7 kB 1.2 MB/s eta 0:00:01\n",
      "     ----------------                       307.2/721.7 kB 1.9 MB/s eta 0:00:01\n",
      "     --------------------------             501.8/721.7 kB 2.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 721.7/721.7 kB 2.8 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\juanj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym) (1.23.5)\n",
      "Collecting cloudpickle>=1.2.0 (from gym)\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting gym-notices>=0.0.4 (from gym)\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml): started\n",
      "  Building wheel for gym (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827649 sha256=1d436ad10119048a1892ab571f4bf551ca94f9db7b99958a93afd31913b2a90e\n",
      "  Stored in directory: c:\\users\\juanj\\appdata\\local\\pip\\cache\\wheels\\b9\\22\\6d\\3e7b32d98451b4cd9d12417052affbeeeea012955d437da1da\n",
      "Successfully built gym\n",
      "Installing collected packages: gym-notices, cloudpickle, gym\n",
      "Successfully installed cloudpickle-2.2.1 gym-0.26.2 gym-notices-0.0.8\n"
     ]
    }
   ],
   "source": [
    "pip install gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym #cargamos la libreria gym\n",
    "\n",
    "envi = gym.make(\"MountainCar-v0\") #lanzamos una instancia de videojuego de la montaña rusa\n",
    "envi.reset() #limpiar y reparar el entorno para toamr deciciones\n",
    "for _ in range(2000): #se realizaran 2000 iteraciones (2000 veces el proceso)\n",
    "    envi.render() #se renderizara las acciones en pantalla \n",
    "    envi.step(envi.action_space.sample()) #toma de decicion aleatoria del conjunto de las opciones disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

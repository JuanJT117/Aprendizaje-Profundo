{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">#Readme--> El repositorio del curso de inteligencia artificial\n",
    ">Para futuras referencias el repositorio del curso se encuentra alojado en la siguiente liga de GitHub, da click en el URL https://github.com/joanby/ia-course "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduccion\n",
    "## 1.1. Aprendizaje por refuerso \n",
    "\n",
    "En este tipo de aprendizaje se se ruefuerza la accion correcta, de esta forma reforzamos (aprender) la accion correcta, de la mismamanera que se entrena a un mascota animando el comportamiento deseado y penalizando el comportamiento negativo, cada accion recibe una respuesta / recompensa en funcion de su desempeño generando la tarea objetivo, el aprendizaje por refuerzo ayuda a generar conocimiento y de esta forma va aprendiendo y mejorando cada aspecto de su aprendizaje.\n",
    "\n",
    "Estricatamente el Prendizaje por refuerzo es un area del aprendizaje automatico inspirado en la psicologia conductista, cuya ocupacion es determinar que accion debe escojer un agente de software en un entorno definido con el fin de maximizar alguna nocuin de \"recompenza\" o \"premio\" acumulado. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Ecuacion de BELLMAN\n",
    "\n",
    "Notaciones:\n",
    "\n",
    "- s -> estados\n",
    "- a -> acciones\n",
    "  - son las acicones que toma el algoridmo para llegar a su objetivo\n",
    "- R -> recompensas\n",
    "  -  Premio y castigo/penailzacion\n",
    "- γ -> descuento\n",
    "\n",
    ">Esta es la ecuacion de BELLMAN\n",
    "> Referencia URL: https://www.ams.org/journals/bull/1954-60-06/S0002-9904-1954-09848-8/S0002-9904-1954-09848-8.pdf\n",
    ">\n",
    ">$$ \n",
    ">V(s)=\\max _{a_{0}}(R(s,a)+γV(s')) \n",
    ">$$ (1)\n",
    ">\n",
    ">De momento parece que solo hemos hecho el problema más complicado al separar la decisión de hoy de las decisiones futuras. Pero podemos simplificar por darse cuenta de que lo que está dentro de los corchetes de la derecha es el valor del tiempo de problema de decisión 1, a partir de un estado\n",
    ">\n",
    ">$$ s_1=T(s_0,a_0)$$\n",
    ">\n",
    ">Por lo tanto se puede reescribir el problema como un recurrente definición de la función de valor:\n",
    ">\n",
    ">$$ V(s_{0})=\\max _{a_{0}}\\{R(s_{0},a_{0})+\\gamma V(s_{1})\\}$$\n",
    ">\n",
    ">sujeto a la restricción:\n",
    ">\n",
    ">$$ a_{0}\\in \\Gamma (s_{0}),\\;s_{1}=T(s_{0},a_{0}).$$\n",
    ">\n",
    ">Esta es la ecuación de Bellman. Se puede simplificar aún más si se cae subíndices de tiempo y el enchufe en el valor del siguiente estado:\n",
    ">\n",
    ">$$ V(s)=\\max _{a\\in \\Gamma (s)}\\{R(s,a)+\\gamma V(T(s,a))\\}.$$\n",
    ">\n",
    ">La ecuación de Bellman se clasifica como una ecuación funcional, porque resolver que significa la búsqueda de la función desconocida V, que es la función de valor. Recordemos que la función de valor describe el mejor valor posible del objetivo, como una función del estado s. Mediante el cálculo de la función de valor, también se encuentra la función a (s) que describe la acción óptima en función de la situación, lo que se llama a la función política.\n",
    "\n",
    "El Principio de optimalidad de Bellman es un principio aplicado en programación dinámica que consiste en que una secuencia óptima de decisiones que resuelve un problema debe cumplir la propiedad de que cualquier subsecuencia de decisiones, que tenga el mismo estado final, debe ser también óptima respecto al subproblema correspondiente. En otras palabras, una política óptima tiene la propiedad de que cualquiera que sea el estado inicial y la decisión inicial, las decisiones restantes deben constituir una política óptima en relación con el estado resultante de la primera decisión.\n",
    "\n",
    "Un ejemplo sencillo del Principio de Optimalidad de Bellman es el siguiente: si tenemos un camino mínimo de A a B pasando por C, entonces los trozos de camino de A a C y de C a B deben ser también mínimos. Esto significa que cualquier subsecuencia de una secuencia óptima debe ser, a su vez, una secuencia óptima "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
